{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#install the necessary packages\n",
        "%pip install pdfplumber\n",
        "%pip install python-docx"
      ],
      "metadata": {
        "id": "kV0PySm7JDwx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5beaf17-2f1d-4abf-b8f4-442e58863279"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.12/dist-packages (0.11.8)\n",
            "Requirement already satisfied: pdfminer.six==20251107 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (20251107)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (5.1.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251107->pdfplumber) (3.4.4)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251107->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (6.0.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "nQoKdrOHImNn"
      },
      "outputs": [],
      "source": [
        "#import the necessary libraries\n",
        "import os\n",
        "#refer to the \"create an ats scanner\" notes\n",
        "import pdfplumber\n",
        "#python-docx\n",
        "#Link: https://python-docx.readthedocs.io/en/latest/\n",
        "from docx import Document\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parser.py"
      ],
      "metadata": {
        "id": "btw2u9DCcQzK"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_file_type(filepath):\n",
        "  file_ext = os.path.splitext(filepath)[1].lower()\n",
        "  #return file_ext\n",
        "  if file_ext == \".pdf\":\n",
        "      return \"pdf\"\n",
        "  elif file_ext == \".docx\":\n",
        "      return \"docx\"\n",
        "  elif file_ext == \".txt\":\n",
        "      return \"txt\"\n",
        "  else:\n",
        "      return \"Unsupported resume format. Use either a .PDF, .DOCX, or .TXT file.\"\n",
        "\n",
        "def parse_pdf(filepath):\n",
        "  text = \"\"\n",
        "  with pdfplumber.open(filepath) as pdf:\n",
        "    for page in pdf.pages:\n",
        "      text += page.extract_text()\n",
        "  return text\n",
        "\n",
        "def parse_docx(filepath):\n",
        "  text = \"\"\n",
        "  doc = Document(filepath)\n",
        "  for paragraph in doc.paragraphs:\n",
        "    text += paragraph.text\n",
        "  return text\n",
        "\n",
        "def parse_txt(filepath):\n",
        "  with open(filepath, \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "'''\n",
        "def clean_text(text):\n",
        "  text = text.replace(\"\\n\", \" \")\n",
        "'''\n",
        "\n",
        "def clean_text(text):\n",
        "  #step 1:\n",
        "  #step 1a. - remove \\r\n",
        "  cleaned = text.replace(\"\\r\", \"\")\n",
        "  #step 1b. - replace \\t with a space\n",
        "  cleaned = cleaned.replace(\"\\t\", \" \")\n",
        "\n",
        "  #step 2: replace the 3 main bullet point types with a - and a trailing space\n",
        "  #Ex. • Built SQL pipelines becomes - Built SQL pipelines\n",
        "  cleaned = cleaned.replace(\"•\", \"- \")\n",
        "  cleaned = cleaned.replace(\"●\", \"- \")\n",
        "  cleaned = cleaned.replace(\"\", \"- \")\n",
        "\n",
        "  #update 1/2 need to be worked on\n",
        "  #update 1: put email, phone and link on separate lines\n",
        "  #cleaned = re.sub(r\"\\s*-\\s*\", \"\\n\", cleaned)\n",
        "  #update 2: put job titles and employment date ranges on separate lines\n",
        "  #cleaned = re.sub(r\"(\\b\\d{4}\\b.*)\", r\"\\n\\1\", cleaned)\n",
        "\n",
        "  #step 3:\n",
        "  #step 3a. - split it into lines\n",
        "  lines = [line.strip() for line in cleaned.split(\"\\n\")]\n",
        "  #step 3b. - remove non-empty lines\n",
        "  lines = [line for line in lines if line]\n",
        "\n",
        "  cleaned = \"\\n\".join(lines)\n",
        "  return cleaned\n",
        "\n",
        "#main function that combines the previous functions\n",
        "def parse_resume(filepath):\n",
        "  #function 1\n",
        "  file_type = detect_file_type(filepath)\n",
        "  #functions 2,3,4\n",
        "  if file_type == \"pdf\":\n",
        "      raw_text = parse_pdf(filepath)\n",
        "  elif file_type == \"docx\":\n",
        "      raw_text = parse_docx(filepath)\n",
        "  elif file_type == \"txt\":\n",
        "      raw_text = parse_txt(filepath)\n",
        "  else:\n",
        "      raise ValueError(\"Unsupported resume format. Use either a .PDF, .DOCX, or .TXT file.\")\n",
        "\n",
        "  cleaned = clean_text(raw_text)\n",
        "\n",
        "  return {\n",
        "      \"file_name\": os.path.basename(filepath),\n",
        "      \"file_type\": file_type,\n",
        "      \"raw_text\": raw_text,\n",
        "      \"clean_text\": cleaned\n",
        "  }"
      ],
      "metadata": {
        "id": "JYPVDaSDJD3U"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#detect_file_type(\"example.pdf\")\n",
        "#parse_pdf(\"example.pdf\")\n",
        "#clean_text(parse_pdf(\"example.pdf\"))\n",
        "#print(clean_text(parse_pdf(\"example.pdf\")))\n",
        "#parse_resume(\"example.pdf\")"
      ],
      "metadata": {
        "id": "y8nDJiMNK1nd"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#jd_parser.py"
      ],
      "metadata": {
        "id": "MVQcC7eILTqK"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the necessary libraries\n",
        "import os\n",
        "import re"
      ],
      "metadata": {
        "id": "zEkfxX2Mj0-B"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_jd(filepath):\n",
        "  with open(filepath, \"rb\") as f:\n",
        "    raw = f.read()\n",
        "\n",
        "  try:\n",
        "    return raw.decode(\"utf-8\")\n",
        "  except UnicodeDecodeError:\n",
        "    pass\n",
        "\n",
        "  try:\n",
        "    return raw.decode(\"cp1252\")\n",
        "  except UnicodeDecodeError:\n",
        "    pass\n",
        "\n",
        "  return raw.decode(\"utf-8\", errors=\"replace\")\n",
        "\n",
        "#normalize and clean the text\n",
        "def clean_jd_text(text):\n",
        "  #step 1:\n",
        "  #step 1a. - remove \\r\n",
        "  cleaned = text.replace(\"\\r\", \"\")\n",
        "  #step 1b. - replace \\t with a space\n",
        "  cleaned = cleaned.replace(\"\\t\", \" \")\n",
        "\n",
        "  #step 2: replace the 3 main bullet point types with a - and a trailing space\n",
        "  #Ex. • Built SQL pipelines becomes - Built SQL pipelines\n",
        "  cleaned = cleaned.replace(\"•\", \"- \")\n",
        "  cleaned = cleaned.replace(\"●\", \"- \")\n",
        "  cleaned = cleaned.replace(\"\", \"- \")\n",
        "\n",
        "  #update 1/2 need to be worked on\n",
        "  #update 1: put email, phone and link on separate lines\n",
        "  #cleaned = re.sub(r\"\\s*-\\s*\", \"\\n\", cleaned)\n",
        "  #update 2: put job titles and employment date ranges on separate lines\n",
        "  #cleaned = re.sub(r\"(\\b\\d{4}\\b.*)\", r\"\\n\\1\", cleaned)\n",
        "\n",
        "  #step 3:\n",
        "  #step 3a. - split it into lines\n",
        "  lines = [line.strip() for line in cleaned.split(\"\\n\")]\n",
        "  #step 3b. - remove non-empty lines\n",
        "  lines = [line for line in lines if line]\n",
        "\n",
        "  cleaned = \"\\n\".join(lines)\n",
        "  return cleaned\n",
        "\n",
        "#grab the job title\n",
        "def extract_job_title(text):\n",
        "  first_line = text.split(\"\\n\")[0]\n",
        "  job_title = first_line.strip()\n",
        "  return job_title\n",
        "\n",
        "#identify the required and preferred skills - from my own personal experience\n",
        "def extract_skills(text):\n",
        "  required = []\n",
        "  preferred = []\n",
        "  return {\n",
        "      \"required_skills\": required,\n",
        "      \"preferred_skills\": preferred,\n",
        "      \"all_skills\": required + preferred\n",
        "  }\n",
        "\n",
        "#grab the experience needed for the role - years and seniority\n",
        "def extract_experience_requirements(text):\n",
        "  experience_years = None\n",
        "  seniority = None\n",
        "  return {\n",
        "      \"experience_years\": experience_years,\n",
        "      \"seniority\": seniority\n",
        "  }\n",
        "\n",
        "#extract job responsibilities\n",
        "def extract_job_responsibilities(text):\n",
        "  responsibilities = []\n",
        "  for line in text.split(\"\\n\"):\n",
        "    if line.startswith(\"-\") or line.startswith(\"*\"):\n",
        "      responsibilities.append(line)\n",
        "  return responsibilities\n",
        "\n",
        "#main function that combines the previous functions\n",
        "def parse_jd(filepath):\n",
        "    raw_text = load_jd(filepath)\n",
        "    clean_text = clean_jd_text(raw_text)\n",
        "\n",
        "    title = extract_job_title(clean_text)\n",
        "    skill_data = extract_skills(clean_text)\n",
        "    experience_data = extract_experience_requirements(clean_text)\n",
        "    responsibilities = extract_job_responsibilities(clean_text)\n",
        "\n",
        "    return {\n",
        "        \"file_name\": os.path.basename(filepath),\n",
        "        \"raw_text\": raw_text,\n",
        "        \"clean_text\": clean_text,\n",
        "        \"job_title\": title,\n",
        "        \"required_skills\": skill_data[\"required_skills\"],\n",
        "        \"preferred_skills\": skill_data[\"preferred_skills\"],\n",
        "        \"all_skills\": skill_data[\"all_skills\"],\n",
        "        \"responsibilities\": responsibilities,\n",
        "        \"min_experience_years\": experience_data[\"experience_years\"],\n",
        "        \"seniority\": experience_data[\"seniority\"]\n",
        "    }"
      ],
      "metadata": {
        "id": "kJVv1gNqksK4"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = load_jd(\"sample_jd.pdf\")\n",
        "#print(text)"
      ],
      "metadata": {
        "id": "_TUc9HIJlqJB"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = clean_jd_text(text)\n",
        "#print(cleaned_text)"
      ],
      "metadata": {
        "id": "_F8pE98rowA3"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_title = extract_job_title(cleaned_text)\n",
        "#print(job_title)"
      ],
      "metadata": {
        "id": "-cUvmD-3pDuq"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_skills = extract_skills(cleaned_text)\n",
        "#print(job_skills)"
      ],
      "metadata": {
        "id": "UpjcLlJqp5U8"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_experience_requirements = extract_experience_requirements(cleaned_text)\n",
        "#print(job_experience_requirements)"
      ],
      "metadata": {
        "id": "MYYd2cIUqJd7"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_resposibilities = extract_job_responsibilities(cleaned_text)\n",
        "#print(job_resposibilities)"
      ],
      "metadata": {
        "id": "V3xMExFjqUmT"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_result = parse_jd(\"sample_jd.pdf\")\n",
        "#print(job_result)"
      ],
      "metadata": {
        "id": "xkw74i-IqfBp"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#job description fixes"
      ],
      "metadata": {
        "id": "xmTbC1XlvWsC"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#grab the job title\n",
        "def extract_job_title(text):\n",
        "  first_line = text.split(\"\\n\")[0]\n",
        "  job_title = first_line.strip()\n",
        "  return job_title"
      ],
      "metadata": {
        "id": "y3PEEMXhqr4I"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#based on the sample_jd_1.txt file, the first line contains multiple pieces of information: job title, city, st ate, country, etc.\n",
        "#the extract_job_title(text): assumes that the first line = job title\n",
        "#we need to return only the part before the location/extra text\n",
        "def extract_job_title(text):\n",
        "  first_line = text.split(\"\\n\")[0]\n",
        "  markers = [\" Job Description\",\" Job Summary\", \" United States\", \", IL\", \", CA\", \", TX\", \", NY\", \" and \"]\n",
        "  for marker in markers:\n",
        "    if marker in first_line:\n",
        "      first_line = first_line.split(marker)[0]\n",
        "  job_title = first_line.strip()\n",
        "  return job_title"
      ],
      "metadata": {
        "id": "hYGrJlV7vTOu"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_title = extract_job_title(cleaned_text)\n",
        "print(job_title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WQo27DpzSEl",
        "outputId": "b4b4eb07-9717-4cd7-d1e3-8e475266c8e7"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "%PDF-1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import the necessary libraries\n",
        "#Link: https://course.spacy.io/en/\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def extract_job_title_spacy(text):\n",
        "  doc = nlp(text)\n",
        "  matcher = Matcher(nlp.vocab)\n",
        "\n",
        "  #add more patterns accordingly\n",
        "  #pattern 1: adj. + noun + noun + proper noun\n",
        "  #Ex. Senior Data Visualization Specialist\n",
        "  pattern1 = [\n",
        "      {\"POS\": \"ADJ\"},\n",
        "      {\"POS\": \"NOUN\"},\n",
        "      {\"POS\": \"NOUN\"},\n",
        "      {\"POS\": \"PROPN\"}\n",
        "  ]\n",
        "  #pattern 2: Ex. Senior Machine Learning Engineer\n",
        "  pattern2 = [\n",
        "      {\"POS\": \"ADJ\"},\n",
        "      {\"POS\": \"PROPN\"},\n",
        "      {\"POS\": \"PROPN\"}\n",
        "  ]\n",
        "  #pattern 3: Ex. Data Visualization Specialist\n",
        "  pattern3 = [\n",
        "      {\"POS\": \"NOUN\"},\n",
        "      {\"POS\": \"NOUN\"},\n",
        "      {\"POS\": \"PROPN\"}\n",
        "  ]\n",
        "  #pattern 4\n",
        "  pattern4 = [\n",
        "      {\"POS\": \"ADJ\"},\n",
        "      {\"POS\": \"PROPN\"},\n",
        "      {\"POS\": \"PROPN\"},\n",
        "      {\"POS\": \"PROPN\"}\n",
        "  ]\n",
        "\n",
        "  '''\n",
        "  #combine the patterns into one\n",
        "  pattern_flexible = [\n",
        "      {\"POS\": {\"IN\": [\"ADJ\", \"PROPN\"]}, \"OP\": \"?\"},\n",
        "      {\"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}},\n",
        "      {\"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}, \"OP\": \"*\"}\n",
        "    ]\n",
        "  '''\n",
        "\n",
        "  matcher.add(\"JOB_TITLE\", [pattern1, pattern2, pattern3, pattern4])\n",
        "  matches = matcher(doc)\n",
        "\n",
        "  #matcher.add(\"JOB_TITLE\", [pattern_flexible])\n",
        "  #matches = matcher(doc)\n",
        "\n",
        "  for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    return span.text.strip()\n",
        "\n",
        "  return None"
      ],
      "metadata": {
        "id": "PsvskD8qzU-m"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_2 = \"Senior Data Visualization Specialist Oak Brook, IL, United States and 2 more Job Description Job Summary:\"\n",
        "job_title_2 = extract_job_title_spacy(text_2)\n",
        "print(job_title_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NDhdcfH082V",
        "outputId": "6064ccd2-d778-480e-baea-391351be8007"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "more Job Description\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_string = \"Senior Data Visualization Specialist Oak Brook, IL, United States and 2 more Job Description Job Summary:\"\n",
        "\n",
        "words = my_string.split()\n",
        "#first_four_words = words[:4]\n",
        "first_four_words = \" \".join(words[:4])\n",
        "print(first_four_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzzFY0VKWpab",
        "outputId": "0ab1ac91-d041-4950-a573-65e8d1b2376a"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Senior Data Visualization Specialist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#keyword_extractor.py"
      ],
      "metadata": {
        "id": "ZK4M1jtpoTSJ"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skills_library = {\n",
        "    \"programming\": [\"python\", \"java\", \"sql\", \"r\"],\n",
        "    \"cloud\": [\"aws\", \"azure\", \"gcp\"],\n",
        "    \"data_tools\": [\"power bi\", \"tableau\", \"excel\", \"snowflake\"],\n",
        "    \"ml_tools\": [\"scikit-learn\", \"tensorflow\", \"pytorch\"],\n",
        "    \"databases\": [\"postgresql\", \"mysql\", \"oracle\"],\n",
        "    \"soft_skills\": [\"communication\", \"leadership\", \"collaboration\"]\n",
        "}\n",
        "\n",
        "all_skills = []\n",
        "for category, items in skills_library.items():\n",
        "    all_skills.extend([skill.lower() for skill in items])\n",
        "#all_skills[:20]\n",
        "\n",
        "#normalize and clean the text\n",
        "def normalize_text(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(r\"[\\n\\r\\t]\", \" \", text)\n",
        "  text = re.sub(r\"[^a-z0-9+\\s\\-\\.]\", \" \", text)\n",
        "  text = re.sub(r\"\\s+\", \" \", text)\n",
        "  return text.strip()\n",
        "\n",
        "#grab skills from resume\n",
        "def find_skills_in_text(text, skills_list):\n",
        "  found = []\n",
        "\n",
        "  #clean_text  = \"python sql aws services\"\n",
        "  #padded_text = \" python sql aws services \"\n",
        "  clean_text = normalize_text(text)\n",
        "  padded_text = f\" {clean_text} \"\n",
        "\n",
        "  for skill in skills_list:\n",
        "    if len(skill) == 1:\n",
        "      if f\" {skill} \" in padded_text:\n",
        "        found.append(skill)\n",
        "      continue\n",
        "    if skill in padded_text:\n",
        "      found.append(skill)\n",
        "\n",
        "  return list(set(found))\n",
        "\n",
        "#grab the required skills from the resume\n",
        "def extract_required_skills_jd(text, skills_list):\n",
        "  required = []\n",
        "  clean_text = normalize_text(text)\n",
        "\n",
        "  required_sections = [\n",
        "    r\"requirements[:\\-\\s]+(.*?)(preferred|nice to have|bonus|$)\",\n",
        "    r\"required skills[:\\-\\s]+(.*?)(preferred|nice to have|bonus|$)\",\n",
        "    r\"must have[:\\-\\s]+(.*?)(preferred|nice to have|bonus|$)\"\n",
        "  ]\n",
        "\n",
        "  for pattern in required_sections:\n",
        "    #re.IGNORECASE - case insensitive\n",
        "    #re.DOTALL - allows multi-line matching\n",
        "    match = re.search(pattern, clean_text, re.IGNORECASE | re.DOTALL)\n",
        "    if match:\n",
        "      section = match.group(1)\n",
        "      required = find_skills_in_text(section, skills_list)\n",
        "      break\n",
        "\n",
        "  return required\n",
        "\n",
        "#grab the preferred skills from the resume\n",
        "def extract_preferred_skills_jd(text, skills_list):\n",
        "  preferred = []\n",
        "  clean_text = normalize_text(text)\n",
        "\n",
        "  preferred_section = [\n",
        "    r\"preferred skills[:\\-\\s]+(.*?)(requirements|required|must have|$)\",\n",
        "    r\"preferred qualifications[:\\-\\s]+(.*?)(requirements|required|must have|$)\",\n",
        "    r\"nice to have[:\\-\\s]+(.*?)(requirements|required|must have|$)\",\n",
        "    r\"bonus[:\\-\\s]+(.*?)(requirements|required|must have|$)\"\n",
        "  ]\n",
        "\n",
        "  for pattern in preferred_section:\n",
        "    match = re.search(pattern, clean_text, re.IGNORECASE | re.DOTALL)\n",
        "    if match:\n",
        "      section = match.group(1)\n",
        "      preferred = find_skills_in_text(section, skills_list)\n",
        "      break\n",
        "\n",
        "  return preferred\n",
        "\n",
        "def extract_resume_skills(text, skills_list):\n",
        "  clean_text = normalize_text(text)\n",
        "  resume_skills = find_skills_in_text(clean_text, skills_list)\n",
        "  return resume_skills\n",
        "\n",
        "def compare_skills(required, preferred, resume):\n",
        "  required_found = list(set(required) & set(resume))\n",
        "  required_missing = list(set(required) - set(resume))\n",
        "  preferred_found = list(set(preferred) & set(resume))\n",
        "  preferred_missing = list(set(preferred) - set(resume))\n",
        "\n",
        "  return {\n",
        "    \"required_found\": required_found,\n",
        "    \"required_missing\": required_missing,\n",
        "    \"preferred_found\": preferred_found,\n",
        "    \"preferred_missing\": preferred_missing\n",
        "    }\n",
        "\n",
        "#ATS match percentage based on required skills\n",
        "def compute_skill_match(required, resume):\n",
        "  if len(required) == 0:\n",
        "    return None\n",
        "\n",
        "  required_found = set(required) & set(resume)\n",
        "  match_percent = (len(required_found) / len(required)) * 100\n",
        "\n",
        "  return round(match_percent, 2)\n",
        "\n",
        "#main function that combines the previous functions\n",
        "def extract_keywords(jd_text, resume_text, skills_list):\n",
        "  jd_clean = normalize_text(jd_text)\n",
        "  resume_clean = normalize_text(resume_text)\n",
        "\n",
        "  required = extract_required_skills_jd(jd_clean, skills_list)\n",
        "  preferred = extract_preferred_skills_jd(jd_clean, skills_list)\n",
        "  resume_skills = extract_resume_skills(resume_clean, skills_list)\n",
        "\n",
        "  comparison = compare_skills(required, preferred, resume_skills)\n",
        "  match_percent = compute_skill_match(required, resume_skills)\n",
        "\n",
        "  return {\n",
        "    \"required_skills\": required,\n",
        "    \"preferred_skills\": preferred,\n",
        "    \"resume_skills\": resume_skills,\n",
        "    \"required_found\": comparison[\"required_found\"],\n",
        "    \"required_missing\": comparison[\"required_missing\"],\n",
        "    \"preferred_found\": comparison[\"preferred_found\"],\n",
        "    \"preferred_missing\": comparison[\"preferred_missing\"],\n",
        "    \"skill_match_percent\": match_percent\n",
        "    }"
      ],
      "metadata": {
        "id": "2mMfdQUKtBma"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_text = \"Experienced in Python, SQL, Power BI, and cloud technologies such as AWS.\"\n",
        "skills = find_skills_in_text(resume_text, all_skills)\n",
        "#print(skills)"
      ],
      "metadata": {
        "id": "3cUxMfKttaF8"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "required = [\"python\", \"sql\", \"aws\"]\n",
        "resume = [\"python\", \"sql\"]\n",
        "\n",
        "#compute_skill_match(required, resume)"
      ],
      "metadata": {
        "id": "WWcuia-4vgkO"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scorer.py"
      ],
      "metadata": {
        "id": "1xyae2ar5ka9"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_resume(jd_text, resume_text, skills_list):\n",
        "    from keyword_extractor.ipnyb import extract_keywords\n",
        "\n",
        "    results = extract_keywords(jd_text, resume_text, skills_list)\n",
        "\n",
        "    final_score = results[\"skill_match_percent\"]\n",
        "\n",
        "    return {\n",
        "        \"required_skills\": results[\"required_skills\"],\n",
        "        \"preferred_skills\": results[\"preferred_skills\"],\n",
        "        \"resume_skills\": results[\"resume_skills\"],\n",
        "        \"required_found\": results[\"required_found\"],\n",
        "        \"required_missing\": results[\"required_missing\"],\n",
        "        \"preferred_found\": results[\"preferred_found\"],\n",
        "        \"preferred_missing\": results[\"preferred_missing\"],\n",
        "        \"skill_match_percent\": final_score\n",
        "    }"
      ],
      "metadata": {
        "id": "K3uA76YBXg2S"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0WR_UUjIYNeV"
      },
      "execution_count": 66,
      "outputs": []
    }
  ]
}